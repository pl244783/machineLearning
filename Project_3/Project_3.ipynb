{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 1. Frame the problem\n",
    "Using the customer description, Define the problem your trying to solve in your own words (remember this is not technial but must be specific so the customer understands the project(\n",
    "\n",
    "Given a textfile that contains a message whether that be in a folder or not, we wish to be able to classify the said data into whether it is a spam email, or whether it is a legimate email through the usage of various machine learning models, though namely a multinomial naive bayes model in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 2. Get the Data \n",
    "Define how you recieved the data (provided, gathered..)\n",
    "\n",
    "The data was provided to me by Mr. Rivero within the mail folder as approximately 1000 separate text files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "874 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#uhruhughrughurhg\n",
    "##data structure and common words\n",
    "import os\n",
    "\n",
    "#commenting this out cuz this is to clean my data c:\n",
    "dir = 'mail-Copy1'\n",
    "count = 0\n",
    "\n",
    "for fileName in os.listdir(dir):\n",
    "    f = os.path.join(dir, fileName)\n",
    "#     if 'mail-Copy1/1spm' in str(f):\n",
    "#         os.remove(f)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "print(count, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 3. Explore the Data\n",
    "Gain insights into the data you have from step 2, making sure to identify any bias\n",
    "\n",
    "Potential Bias: Sample size is too little to properly form conclusions, and it does not take into account proper demographics, only for that specific location, and in which case, this scenario would be a college. IT does not factor in the possibility that emails for different targets, for example mothers, or children would have different spam aimed at their specific demographic, in which for this example would be car insurance, or free game hacks. \n",
    "\n",
    "Furthermore, the inclusion of single string characters or single digit values may include some bias, and it may hold no significant value in the overall purpose, as well as containing similarities between the two datasets. For example, the letter 'a' or the number '0' may be used frequently, and can not be used to indicate neither spam nor nonspam. \n",
    "\n",
    "In addition, the sample data itself contains little to no logical sense, in which nonspam emails are sometimes to be considered (at least to the human) more nonsensical than spam. For example, a certain nonspam would be \". . . 's much restrictive s - > np np . 's \" \" np pro quite over-restriction , .\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 874 \n",
      "\n",
      "Top 20 most common words in the directory for SPM:\n",
      "0: 1615\n",
      "email: 1368\n",
      "20: 1306\n",
      "report: 1204\n",
      "s: 1176\n",
      "order: 1168\n",
      "program: 834\n",
      "address: 793\n",
      "mail: 760\n",
      "send: 758\n",
      "our: 706\n",
      "money: 617\n",
      "name: 617\n",
      "receive: 612\n",
      "list: 594\n",
      "3d: 579\n",
      "business: 537\n",
      "free: 498\n",
      "com: 414\n",
      "work: 410\n",
      "\n",
      "\n",
      "Top 20 most common words in the directory for nonSPM:\n",
      "language: 1400\n",
      "university: 925\n",
      "s: 713\n",
      "de: 481\n",
      "conference: 463\n",
      "linguistic: 460\n",
      "0: 457\n",
      "1: 308\n",
      "edu: 298\n",
      "english: 274\n",
      "30: 264\n",
      "e: 237\n",
      "information: 236\n",
      "workshop: 223\n",
      "1998: 220\n",
      "one: 205\n",
      "paper: 203\n",
      "Subject: 196\n",
      "2: 195\n",
      "10: 190\n"
     ]
    }
   ],
   "source": [
    "##data structure and common words\n",
    "import os\n",
    "\n",
    "#commenting this out cuz this is to clean my data c:\n",
    "dir = 'testData'\n",
    "count = 0\n",
    "\n",
    "for fileName in os.listdir(dir):\n",
    "    f = os.path.join(dir, fileName)\n",
    "#     if 'mail-Copy1/1spm' in str(f):\n",
    "#         os.remove(f)\n",
    "\n",
    "    count += 1\n",
    "\n",
    "print('Count:', count, '\\n')\n",
    "\n",
    "import email\n",
    "import email.policy\n",
    "\n",
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def structures_counter(directory, dir):\n",
    "    structures = Counter()\n",
    "    for email in directory:\n",
    "        filePlace = os.path.join(dir, email)\n",
    "        # try:\n",
    "        #     with open(filePlace, 'r', encoding='utf-8') as file:\n",
    "        #         msg = file.read()\n",
    "        #         structure = get_email_structure(msg)\n",
    "        #         structures[structure] += 1\n",
    "        # except IsADirectoryError:\n",
    "        #     pass\n",
    "        for email in emails:\n",
    "            structure = get_email_structure(email)\n",
    "            structures[structure] += 1\n",
    "    return structures\n",
    "\n",
    "#STINKY ALERT, DO NOT UNCOMMENT THIS, IT WILL BE VERY BAD\n",
    "# print(structures_counter(os.listdir(dir), dir).most_common())\n",
    "\n",
    "#------------------------------------------------------------\n",
    "\n",
    "import glob\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def commonWords(text, n):\n",
    "    # Define a regex pattern to match all punctuation characters\n",
    "    punctuation = '[!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'\n",
    "    #comment this in and out as needbe\n",
    "    # nmb = '[0-9]+'\n",
    "    \n",
    "    punctuationNt = re.sub(punctuation, '', text)\n",
    "    # nmbNt = re.sub(nmb, '', punctuationNt)\n",
    "    # words = nmbNt.split()\n",
    "    words = punctuationNt.split()\n",
    "    pogWords = Counter(words)\n",
    "    topWords = pogWords.most_common(n)\n",
    "    return topWords\n",
    "\n",
    "def processor(path, n):\n",
    "    with open(path, 'rt') as file:\n",
    "        return commonWords(file.read(), n)\n",
    "\n",
    "def n1(dir, n):\n",
    "    SPMcommonWords = Counter()\n",
    "    nonSPMcommonWords = Counter()\n",
    "    \n",
    "    for filename in os.listdir(dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            if filename[0] == '1':\n",
    "                SPMpath = os.path.join(dir, filename)\n",
    "                SPMtopWords = processor(SPMpath, n)\n",
    "                SPMcommonWords.update(dict(SPMtopWords))\n",
    "            else:\n",
    "                nonSPMpath = os.path.join(dir, filename)\n",
    "                nonSPMtopWords = processor(nonSPMpath, n)\n",
    "                nonSPMcommonWords.update(dict(nonSPMtopWords))\n",
    "\n",
    "    print(f\"Top {n} most common words in the directory for SPM:\")\n",
    "    for word, count in SPMcommonWords.most_common(n):\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "    print('\\n\\n'+f\"Top {n} most common words in the directory for nonSPM:\")\n",
    "    for word, count in nonSPMcommonWords.most_common(n):\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "n1('testData', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Prepare the Data\n",
    "\n",
    "\n",
    "Apply any data transformations and explain what and why\n",
    "\n",
    "I did the following steps: \n",
    "\n",
    "I edited all my data, dropping all forms of punctuation. Things like \"!\" or \".\" don't serve much relevant purpose towards spam detection, as saying \"This is SPAM!!!!\" and \"This is spam.\" convey the same meaning. \n",
    "\n",
    "After doing so, I renamed all my files to start with either 0 or 1 to convey whether it's non spam or spam. This is namely to help my model further along when I start comparing the data with the title in order to check if the predicted answer is correct. \n",
    "\n",
    "After doing so, I lemminated all text and saved it to a list. This is mainly done to make sure that when I do a word to vector transition in the next part, similar words with the same root are considered the same thing. For example, \"I am very happy\" vs \"I am happier\", happy and happier could be classified both as happy, in order to improve accuracy.\n",
    "\n",
    "Finally I vectorized my words to an array, using the top 50 most common words as features. This is namely for future work with models, as they only can work with numbers. ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretically, hopefully, not broken done\n",
      "Count: 874 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#probably drop numbers and single letters because they aren't useful\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "#---------------------------------------------------------\n",
    "\n",
    "#THE UH OH STINKY BUTTON\n",
    "def uhOhStinky():\n",
    "    cd = os.getcwd()\n",
    "    for f in os.listdir(cd):\n",
    "        if f.endswith(\".txt\"):\n",
    "            z = os.path.join(cd, f)\n",
    "            os.remove(z)\n",
    "\n",
    "# uhOhStinky()\n",
    "\n",
    "#stinky deleter 2\n",
    "import shutil\n",
    "# shutil.rmtree('testData')\n",
    "\n",
    "#---------------------------------------------------------\n",
    "\n",
    "#processor reset chucking this here for the future\n",
    "\n",
    "#temp code, I'll probably fix the variables if I don't get lazy\n",
    "def n2(dir):\n",
    "    for n in os.listdir(dir): \n",
    "        z = os.path.join(dir, n)\n",
    "        if z.endswith('.txt'):    \n",
    "            with open(z, 'r') as f:\n",
    "                q = f.read()\n",
    "                punctuation = '[!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'\n",
    "                qq = re.sub(punctuation, '', q)\n",
    "                f.close()\n",
    "    \n",
    "            with open(z, 'w') as g:\n",
    "                g.write(qq)\n",
    "                g.close()\n",
    "\n",
    "n2('testData')\n",
    "\n",
    "print('Theoretically, hopefully, not broken done')\n",
    "\n",
    "count = 0\n",
    "for fileName in os.listdir('testData'):\n",
    "    # f = os.path.join(dir, fileName)\n",
    "#     if 'mail-Copy1/1spm' in str(f):\n",
    "#         os.remove(f)\n",
    "    count += 1\n",
    "\n",
    "print('Count:', count, '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poggies?? Done\n"
     ]
    }
   ],
   "source": [
    "#processor part 2: Labeling data\n",
    "\n",
    "#---------------------------------------------------------\n",
    "\n",
    "#the stinky removinator:\n",
    "#shutil.rmtree('testData2')\n",
    "\n",
    "#---------------------------------------------------------\n",
    "import os\n",
    "\n",
    "def n3(dir):\n",
    "    arr = ['0', '1']\n",
    "    for n in os.listdir(dir):\n",
    "        z = os.path.join(dir, n)\n",
    "        if z.endswith('.txt') and n[0] not in arr:\n",
    "            if 'spm' in z:\n",
    "                os.rename(z, dir+'/1'+n)\n",
    "            else: \n",
    "                os.rename(z, dir+'/0'+n)\n",
    "\n",
    "n3('testData')\n",
    "\n",
    "print('Poggies?? Done')\n",
    "\n",
    "#---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10' '100' '20' '30' '3d' '50' 'address' 'business' 'call' 'check' 'com'\n",
      " 'conference' 'day' 'de' 'each' 'edu' 'email' 'english' 'even' 'fax'\n",
      " 'first' 'follow' 'form' 'free' 'here' 'http' 'include' 'information'\n",
      " 'interest' 'internet' 'language' 'letter' 'linguistic' 'list' 'mail'\n",
      " 'many' 'market' 'money' 'most' 'name' 'need' 'nt' 'number' 'offer' 'one'\n",
      " 'order' 'our' 'over' 'paper' 'please' 'product' 'program' 'read'\n",
      " 'receive' 'report' 'send' 'service' 'site' 'start' 'subject' 'system'\n",
      " 'those' 'university' 'us' 'want' 'web' 'week' 'work' 'www'] \n",
      "\n",
      "[[0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [1 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "  value                                               text\n",
      "0     0  [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 0, ...\n",
      "1     0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "2     0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "3     0  [0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 5, 1, ...\n",
      "4     0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "    value                                               text\n",
      "869     0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "870     1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, ...\n",
      "871     0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...\n",
      "872     0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, ...\n",
      "873     0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cv.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final transformation work i guess\n",
    "#todo: \n",
    "#set it to a vector transformer, where the first 20 words are known\n",
    "#also stem words ig\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "#data stemming\n",
    "#nltk.download('wordnet')\n",
    "l = WordNetLemmatizer()\n",
    "\n",
    "def n4(dir, q):\n",
    "    if q.endswith('.txt'):\n",
    "        with open(os.path.join(dir, q), 'r') as f:\n",
    "            t = f.read()\n",
    "            tL = nltk.word_tokenize(t)\n",
    "            return ' '.join(tL)\n",
    "            # return t\n",
    "\n",
    "x = [n4('testData2', f) for f in os.listdir('testData2')]\n",
    "y = [f[0] for f in os.listdir('testData2')] #where f[0] is 0 or 1\n",
    "\n",
    "#print(x[6], '\\n\\n', y[6], '\\n\\n', len(x), len(y))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(analyzer='word', max_features = 69)\n",
    "x = cv.fit_transform([str(y) for y in x if y is not np.nan])\n",
    "\n",
    "print(cv.get_feature_names_out(), '\\n')\n",
    "print(x.toarray())\n",
    "\n",
    "\n",
    "#converting to a df\n",
    "dF = pd.DataFrame({'value': [y[v] for v in range(len(y))],\n",
    "                  'text': [x.toarray()[v] for v in range(len(y))]})\n",
    "\n",
    "print(dF.head())\n",
    "print(dF.tail())\n",
    "\n",
    "xTr, XTe, YTr, YTe = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "joblib.dump(cv, 'cv.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it would be really funny to have a pipeline\n",
    "#leaving this here, not to do anyting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model the data\n",
    "Using selected ML models, experment with your choices and describe your findings. Finish by selecting a Model to continue with\n",
    "\n",
    "I used multinomial naive bayes, as that is one of the most popular models to use in regards to natural language processing. Seeing that I got a 96% accuracy, I see it fit to move on to fine tuning and seeing what best would aid my work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['m.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import joblib\n",
    "\n",
    "m = MultinomialNB()\n",
    "m.fit(xTr, YTr)\n",
    "print(\"Accuracy: \", m.score(XTe, YTe)*100, '%', sep='')\n",
    "\n",
    "joblib.dump(m, \"m.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fine Tune the Model\n",
    "\n",
    "With the select model descibe the steps taken to acheve the best rusults possiable \n",
    "\n",
    "I gridsearched the results using the possible parameters without breaking my model. I used cv=5 because that'd let me 5fold score, giving me the best possible result. The only issue that occured is that MultinomialNB may only contain 1 parameter, which is alpha. It doesn't do much, making it largely useless and the gridsearch irrelevant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-michaelw/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'alpha': [0.1, 0.5, 1.0, 2.0]}\n",
    "grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5)\n",
    "grid_search.fit(xTr, YTr)\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "print(best_alpha)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Present\n",
    "In a customer faceing Document provide summery of finding and detail approach taken\n",
    "\n",
    "\n",
    "1. Upon Recieving the Data, it was immediately converted to a readable format for the machine. This was largely done by first removing all punctuation, labeling the data, and finally, converting the words to a readable format (namely numbers) for the machine.  \n",
    "2. The model was then trained using a Multinomial Naive Bayes model, which turned out to be quite successful. This is largely due to the relatively obvious split in the data as demonstrated in Step 3 where we looked for patterns.\n",
    "3. Upon gridsearching it --a method in which we try to find the best possible parameters to improve our model--, we were unable to significantly improve it, leaving the accuracy at 96%. If we were to improve, we'd most likely try to gather more data, as gridsearching is likely to not improve things due to the singular parameter. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Launch the Model System\n",
    "Define your production run code, This should be self susficent and require only your model pramaters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Predicted Value for File 1: Non-Spam\n",
      "2. Predicted Value for File 2: Non-Spam\n",
      "3. Predicted Value for File 3: Non-Spam\n",
      "4. Predicted Value for File 4: Non-Spam\n",
      "5. Predicted Value for File 5: Non-Spam\n",
      "6. Predicted Value for File 6: Non-Spam\n",
      "7. Predicted Value for File 7: Spam\n",
      "8. Predicted Value for File 8: Non-Spam\n",
      "9. Predicted Value for File 9: Non-Spam\n",
      "10. Predicted Value for File 10: Non-Spam\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Print Limit Hit. Enter Y to display all Results, and C to display the next 10 Predictions c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "12. Predicted Value for File 12: Non-Spam\n",
      "13. Predicted Value for File 13: Non-Spam\n",
      "14. Predicted Value for File 14: Spam\n",
      "15. Predicted Value for File 15: Spam\n",
      "16. Predicted Value for File 16: Non-Spam\n",
      "17. Predicted Value for File 17: Non-Spam\n",
      "18. Predicted Value for File 18: Non-Spam\n",
      "19. Predicted Value for File 19: Non-Spam\n",
      "20. Predicted Value for File 20: Spam\n"
     ]
    }
   ],
   "source": [
    "def n7(*args):\n",
    "    import os, re, joblib, nltk\n",
    "    import numpy as np\n",
    "\n",
    "    x = []\n",
    "    \n",
    "    def n2(dir, z): \n",
    "        if os.path.isdir(dir):\n",
    "            z = os.path.join(dir, z)\n",
    "            \n",
    "        if z.endswith('.txt'):\n",
    "            with open(z, 'r') as f:\n",
    "                q = f.read()\n",
    "                punctuation = '[!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'\n",
    "                qq = re.sub(punctuation, '', q)\n",
    "                f.close()\n",
    "        \n",
    "            with open(z, 'w') as g:\n",
    "                g.write(qq)\n",
    "                g.close()\n",
    "\n",
    "    def n4(dir, z):\n",
    "        if os.path.isdir(dir):\n",
    "            z = os.path.join(dir, z)\n",
    "            \n",
    "        if z.endswith('.txt'):\n",
    "            with open(z, 'r') as f:\n",
    "                t = f.read()\n",
    "                tL = nltk.word_tokenize(t)\n",
    "                return ' '.join(tL)\n",
    "\n",
    "    def n9(x):\n",
    "        cv = joblib.load('cv.pkl')\n",
    "        m = joblib.load('m.pkl')\n",
    "\n",
    "        x = cv.fit_transform([str(y) for y in x if y is not np.nan])\n",
    "        \n",
    "        c, l = 0, 10\n",
    "        for y in x:\n",
    "            pV = m.predict(y)\n",
    "\n",
    "            if pV[0] == '1':\n",
    "                aN = 'Spam'\n",
    "            else:\n",
    "                aN = 'Non-Spam'\n",
    "                \n",
    "            if c != -1:\n",
    "                c+= 1\n",
    "                \n",
    "            if c <= l:\n",
    "                print('{}. Predicted Value for File {}:'. format(c, c), aN)\n",
    "            else:\n",
    "                u = str(input('\\nPrint Limit Hit. Enter Y to display all Results, and C to display the next 10 Predictions')).lower()\n",
    "                print('\\n')\n",
    "\n",
    "                if u == 'y':\n",
    "                    c = -1\n",
    "                elif u == 'c':\n",
    "                    l += 10\n",
    "                else: \n",
    "                    return 0\n",
    "    \n",
    "    \n",
    "    #we're going to assume that our arguement is a file path\n",
    "    for n in args:\n",
    "        n = str(n)\n",
    "        if n.endswith('.txt') == False:\n",
    "            d = False\n",
    "            if os.path.isdir(n) == False:\n",
    "                print('Function Execution Failed. Try Again')\n",
    "                return 0\n",
    "            else:\n",
    "                d = True\n",
    "                \n",
    "        if d:\n",
    "            for p in os.listdir(n):\n",
    "                n2(n, p)\n",
    "                x.append([n4(n, p)])\n",
    "        else: \n",
    "            n2(n, n)\n",
    "            x.append([n4(n, n)])\n",
    "\n",
    "    n9(x)\n",
    "            \n",
    "n7('wuhwuh')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
