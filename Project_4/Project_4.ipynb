{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 1. Frame the problem\n",
    "Using the customer description, Define the problem your trying to solve in your own words (remember this is not technial but must be specific so the customer understands the project(\n",
    "\n",
    "Given an input image in the format of some sort of file with the extension of .jpg, .png, or the likes and contains a singular cursive letter on it, we wish to be able to classify said input image in a way that'd allow us to distinguish cursive letters from one another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 2. Get the Data \n",
    "Define how you recieved the data (provided, gathered..)\n",
    "\n",
    "The data was provided to me by Mr. Rivero in the format of a google drive link, in which a large folder stored 9 folders, each one containing 26 photoes, one for each letter in cursive. This data would be then downloaded and unzipped within the jupyter notebook itself. \n",
    "\n",
    "The Data was then had all the .HEIC and .HEIF files converted into .JPEG in order to become readable and openable for the average human reader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#FUNNY ZIP :) folder_path=\"testData\" && for file in \"$folder_path\"/*; do [ ${file: -4} == \".zip\" ] && unzip \"$file\" -d \"$folder_path\" && echo \"Unzipped: $file\" || echo \"Skipping non-zip file: $file\"; done\n",
    "\n",
    "\n",
    "#preprocessing and converting the data to a usable format\n",
    "#the stinky removinator:\n",
    "import shutil\n",
    "# shutil.rmtree('copy')\n",
    "\n",
    "#cleaning \n",
    "import os \n",
    "# d = 'testData'\n",
    "# for f in os.listdir(d):\n",
    "#     s = True\n",
    "#     p = os.path.join(d, f)\n",
    "#     if f.endswith('.zip'):\n",
    "#         os.remove(p)\n",
    "#         s = False\n",
    "        \n",
    "#     if s:\n",
    "#         for n in os.listdir(p):\n",
    "#             if 'IMG' in n:\n",
    "                # os.remove(os.path.join(p, n))\n",
    "\n",
    "#2 is an HEIF file, which I'll deal with (checked using \"file [filename]\"\n",
    "from PIL import Image\n",
    "import pillow_heif\n",
    "\n",
    "#swap this back to temp for the whole folder\n",
    "d = 'temp/6'\n",
    "for f in os.listdir(d):\n",
    "    #uncomment this for mass thing, this is commented only to deal with folder 2 and 6\n",
    "    # for n in os.listdir(os.path.join(d, f)):\n",
    "    #     p = os.path.join(os.path.join(d, f), n)\n",
    "        p = os.path.join(d, f)\n",
    "        # print(p)\n",
    "        if (p.endswith('.JPG') == False and os.path.isdir(p) == False):\n",
    "            hF = pillow_heif.read_heif(p)\n",
    "            i = Image.frombytes(\n",
    "                hF.mode,\n",
    "                hF.size,\n",
    "                hF.data.tobytes(),\n",
    "                \"raw\",\n",
    "            \n",
    "            )\n",
    "        \n",
    "            i.save((p[:p.rfind('.')] if '.' in p else p)+\".JPG\", format(\"JPEG\"))\n",
    "            os.remove(p)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 3. Explore the Data\n",
    "Gain insights into the data you have from step 2, making sure to identify any bias\n",
    "\n",
    "Potential Bias: Sample size is most likely too small, as it is only 234 images, with each letter appearing only 9 times. Therefore this data is not indictative of any sort of average writing style nor is conclusive of outlier styles and/or any other unmentioned writing format. For example, a messily written m and n in cursive may be picked up as the same thing\n",
    "\n",
    "Furthermore the data is differently lit, shot at different angles, and cropped differently, all which may affect the final outcome of the prediction as the algorithmn takes these values in. Also handwriting style for a person varies over time and therefore can be considered inconsistent and poor quality, making it quite difficult to make a model with any sort of accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Prepare the Data\n",
    "\n",
    "\n",
    "Apply any data transformations and explain what and why\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#i should crop the images \n",
    "\n",
    "#https://www.digitalocean.com/community/tutorials/how-to-build-a-neural-network-to-recognize-handwritten-digits-with-tensorflow\n",
    "#https://keras.io/examples/vision/handwriting_recognition/\n",
    "import cv2, os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#maybe I'll import a pipeline just this once because I can see the cpu inefficiency\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class midCropper(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self #this is done to make sure it easily fits into scikit learn pipe infrastructure\n",
    "\n",
    "    def transform(self, x): #place where actual data transofrmation takes place\n",
    "        #temp whatever\n",
    "        imgHolder = []\n",
    "\n",
    "        #SOMEONE DIDN'T CROP THEIR THING IT'S 8\n",
    "        #screw it we ball\n",
    "        for n in x:\n",
    "            if not os.path.isdir(n):\n",
    "                img = cv2.imread(n, cv2.IMREAD_GRAYSCALE)\n",
    "                contour, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                    \n",
    "                if contour:\n",
    "                    x1, y1, x2, y2 = cv2.boundingRect(contour[0])\n",
    "                    imgHolder.append({'yV': (os.path.splitext(os.path.basename(n))[0]).lower(),\n",
    "                                      #.flatten() idk\n",
    "                                      'xV': (np.array(Image.fromarray(img[y1:y1 + y2, x1:x2]).resize((500, 500))).flatten())})\n",
    "                    #uncomment this out if you wanna uncomment out the bottom\n",
    "                    # imgHolder.append({(os.path.splitext(os.path.basename(n))[0]).lower(): Image.fromarray(img[y1:y2, x1:x2]).resize((50, 50))})\n",
    "                else:\n",
    "                    print('WEEE WOO WEE WOO ERROR ERROR ERROR', n)\n",
    "\n",
    "        #only return the transformed data please :)\n",
    "        return imgHolder\n",
    "        \n",
    "def processor(inputFolder, outputFolder):\n",
    "    filePath = [os.path.join(os.path.join(inputFolder, innerFolder), fileName) for innerFolder in os.listdir(inputFolder) for fileName in os.listdir(os.path.join(inputFolder, innerFolder))] #every file path i guess\n",
    "\n",
    "    #pipelining time\n",
    "    pipeline = Pipeline([\n",
    "        ('crop', midCropper()),\n",
    "\n",
    "        #saved for when I want to fix it\n",
    "        # ('processor', ColumnTransformer(transformers=[\n",
    "        #     ('scale', StandardScaler(), ['xV'])\n",
    "        # ],\n",
    "        # remainder = 'passthrough'))\n",
    "        \n",
    "        #future stuf fif bug\n",
    "        #future stuff is putting it in a test set lmao\n",
    "    ])\n",
    "\n",
    "    #stuff\n",
    "    output = pipeline.fit_transform(filePath) #data in\n",
    "    # for i, r in enumerate(output): \n",
    "    #     for fileName, imageOutput in r.items():\n",
    "    #         #CHECK HERE TO JOIN OUTPUT PATH FOLDER\n",
    "    #         outputPath = os.path.join(outputFolder, f\"{fileName}{i}.JPG\")\n",
    "    #         imageOutput.save(outputPath)\n",
    "\n",
    "    return output\n",
    "\n",
    "#probably convert it to a vector after :)\n",
    "output = processor('temp', 'test')\n",
    "dF = pd.DataFrame(output)\n",
    "xVM = np.array(list(dF['xV']))\n",
    "xVM = xVM.reshape(xVM.shape[0], -1)\n",
    "XTr, XTe, YTr, YTe = train_test_split(xVM, dF['yV'], test_size=0.15, random_state=42, stratify=dF['yV'])\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  yV                                                 xV\n",
      "0  o  [180, 184, 183, 182, 183, 186, 188, 189, 184, ...\n",
      "1  x  [192, 190, 189, 190, 192, 190, 188, 188, 191, ...\n",
      "2  y  [147, 147, 150, 153, 156, 161, 169, 169, 162, ...\n",
      "3  k  [176, 180, 182, 174, 160, 163, 149, 115, 119, ...\n",
      "4  w  [146, 145, 147, 150, 142, 147, 165, 166, 167, ...\n",
      "\n",
      "\n",
      "\n",
      "[234 234 234 ... 255 255 255]\n",
      "\n",
      "164    j\n",
      "16     i\n",
      "156    o\n",
      "177    b\n",
      "161    a\n",
      "Name: yV, dtype: object\n",
      "\n",
      "\n",
      " 250000 aids \n",
      "\n",
      "\n",
      "[234 234 234 ... 255 255 255]\n"
     ]
    }
   ],
   "source": [
    "#stinky deleter part 2 eletric boogaloo\n",
    "import shutil\n",
    "# shutil.rmtree('test')\n",
    "\n",
    "print(dF.head(), '\\n\\n\\n', sep='')\n",
    "\n",
    "print(XTr[0], '\\n\\n', YTr.head(), sep='')\n",
    "\n",
    "#WEE EWOWO WEOWE  BUG IS THAT IT'S PROBABLY COPMRESSING thE ORIGINAL MIMAIGE NOT THE ORITHIBNG \n",
    "#@#KJ#@KJ JKLFHKJSHF SI FIX ED AND IT STiLL NO WORK OOOMOO\n",
    "print('\\n\\n', len(dF['xV'][76]), 'aids', '\\n\\n')\n",
    "\n",
    "print(XTr[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model the data\n",
    "Using selected ML models, experment with your choices and describe your findings. Finish by selecting a Model to continue with\n",
    "\n",
    "\n",
    "I made a basic neural network, as it is probably the one way to properly predict an output for a given input file. I did two methods, one using a prebuilt model using SKlean and MLPClassifier, and one by trying to build one with keras. Both are extremely bad, which is likely due to an issue with the data itself. I will finetune it just to make sure if it can be salvaged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-michaelw/.local/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import parallel_backend\n",
    "\n",
    "# Assuming XTr, YTr, XTe, YTe are already defined\n",
    "\n",
    "# Create a mapping for the letters\n",
    "letter_mapping = {l: i for i, l in enumerate('abcdefghijklmnopqrstuvwxyz')}\n",
    "\n",
    "# Apply one-hot encoding\n",
    "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "YTrE = encoder.fit_transform(YTr.map(letter_mapping).values.reshape(-1, 1)).astype('int64')\n",
    "YTeE = encoder.transform(YTe.map(letter_mapping).values.reshape(-1, 1)).astype('int64')\n",
    "\n",
    "\n",
    "# Create the MLP classifier\n",
    "m1 = MLPClassifier(\n",
    "    hidden_layer_sizes=(1024, 512, 256, 128, 26),\n",
    "    max_iter=5000,\n",
    "    learning_rate_init=0.001,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Use Joblib for parallel training\n",
    "with parallel_backend('loky', n_jobs=80):\n",
    "    m1.fit(XTr, YTrE)\n",
    "\n",
    "# Evaluate the model\n",
    "YPr = m1.predict(XTe)\n",
    "a = accuracy_score(YTeE, YPr)\n",
    "print(f\"Accuracy: {a}\")\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(YTeE, YPr, average='weighted', zero_division=1)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print('we have achieved liftoff')\n",
    "\n",
    "#BROOKEN DO NOT USE, KEPT CRASHING SERVER :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 15:08:31.609112: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-17 15:08:31.656180: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-17 15:08:31.656209: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-17 15:08:31.656244: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-17 15:08:31.666277: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-17 15:08:32.725252: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108    w\n",
      "72     h\n",
      "9      z\n",
      "142    e\n",
      "1      x\n",
      "Name: yV, dtype: object\n",
      "\n",
      "26\n",
      "\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_1365900/2381861323.py:30: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-michaelw/.local/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 15:08:39.749302: W tensorflow/core/framework/dataset.cc:959] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 7.6227 - accuracy: 0.0404 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 15:11:02.753148: W tensorflow/core/framework/dataset.cc:959] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 169s 24s/step - loss: 7.6227 - accuracy: 0.0404 - val_loss: 7.6280 - val_accuracy: 0.0556\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - ETA: 0s - loss: 7.6280 - accuracy: 0.0354 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 15:13:45.350144: W tensorflow/core/framework/dataset.cc:959] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 139s 20s/step - loss: 7.6280 - accuracy: 0.0354 - val_loss: 7.6280 - val_accuracy: 0.0556\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - ETA: 0s - loss: 7.6280 - accuracy: 0.0354 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 15:16:06.418153: W tensorflow/core/framework/dataset.cc:959] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 141s 20s/step - loss: 7.6280 - accuracy: 0.0354 - val_loss: 7.6280 - val_accuracy: 0.0556\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - ETA: 0s - loss: 7.6280 - accuracy: 0.0354 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 15:18:32.361554: W tensorflow/core/framework/dataset.cc:959] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 146s 21s/step - loss: 7.6280 - accuracy: 0.0354 - val_loss: 7.6280 - val_accuracy: 0.0556\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - ETA: 0s - loss: 7.6280 - accuracy: 0.0354 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 15:21:00.171872: W tensorflow/core/framework/dataset.cc:959] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 148s 21s/step - loss: 7.6280 - accuracy: 0.0354 - val_loss: 7.6280 - val_accuracy: 0.0556\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - ETA: 0s - loss: 7.6280 - accuracy: 0.0354 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 15:23:28.472207: W tensorflow/core/framework/dataset.cc:959] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 173s 25s/step - loss: 7.6280 - accuracy: 0.0354 - val_loss: 7.6280 - val_accuracy: 0.0556\n",
      "we have achieved liftoff\n"
     ]
    }
   ],
   "source": [
    "#https://www.digitalocean.com/community/tutorials/how-to-build-a-neural-network-to-recognize-handwritten-digits-with-tensorflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "tf.config.threading.set_intra_op_parallelism_threads(80)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(80)\n",
    "\n",
    "# map = {l: i for i, l in enumerate('abcdefghijklmnopqrstuvwxyz')}\n",
    "# YTr = YTr.map(map).fillna(0).astype(np.int64)\n",
    "# YTe = YTe.map(map).fillna(0).astype(np.int64)\n",
    "\n",
    "# Create a mapping for the letters\n",
    "letter_mapping = {l: i for i, l in enumerate('abcdefghijklmnopqrstuvwxyz')}\n",
    "\n",
    "# Apply one-hot encoding\n",
    "encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "YTrE = encoder.fit_transform(YTr.map(letter_mapping).values.reshape(-1, 1)).astype('int64')\n",
    "YTeE = encoder.transform(YTe.map(letter_mapping).values.reshape(-1, 1)).astype('int64')\n",
    "\n",
    "print(YTr.head(), '\\n\\n', len(YTrE[3]), '\\n\\n', sep='')\n",
    "\n",
    "#apply standard scalar later\n",
    "\n",
    "# m = MLPClassifier(hidden_layer_sizes=(16384, 8192, 4096, 2048, 1024, 512, 256, 128, 64), max_iter=5000, learning_rate_init = 0.001, random_state=42, n_jobs=30)\n",
    "# m.fit(XTr, YTrE)\n",
    "\n",
    "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    m2 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(16384, activation='relu', input_shape=(250000,)),\n",
    "        tf.keras.layers.Dense(4096, activation='relu'),\n",
    "        tf.keras.layers.Dense(1024, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(26, activation='linear')\n",
    "    ])\n",
    "\n",
    "    o = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    m2.compile(optimizer=o,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "eS = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "m2.fit(XTr, YTrE, epochs=10, batch_size=32, validation_data=(XTe, YTeE), callbacks=[eS])\n",
    "\n",
    "# YPr = m.predict(XTe)\n",
    "# a = accuracy_score(YTeE, YPr)\n",
    "# print(f\"Accuracy: {a}\")\n",
    "\n",
    "# from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# precision, recall, f1, _ = precision_recall_fscore_support(YTeE, YPr, average='weighted', zero_division=1)  # Adding zero_division parameter\n",
    "\n",
    "# print(f\"Precision: {precision}\")\n",
    "# print(f\"Recall: {recall}\")\n",
    "# print(f\"F1 Score: {f1}\")\n",
    "\n",
    "print('we have achieved liftoff')\n",
    "#---------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "INFO:tensorflow:Assets written to: m2.pb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: m2.pb/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "#joblib.dump(m1, 'm1.pkl')\n",
    "joblib.dump(m2, 'm2.pkl')\n",
    "print('done')\n",
    "\n",
    "m2.save('m2.pb')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 6. Fine Tune the Model\n",
    "\n",
    "With the select model descibe the steps taken to acheve the best rusults possiable \n",
    "\n",
    "I attempted to finetune model2 to the best of my ability, as it's quicker to run than the MLPClassifier. Unfortunately, it's quite difficult and timeconsuming, so much improvement can not be said given the short time span (It did not improve) Furthermore, the already extremely low accuracy score is unlikely to improve anything drastic. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(4096, 2048, 1024, 512, 256, 128, 64)],\n",
    "    'max_iter': [10000, 5000, 1000],\n",
    "    'learning_rate_init': [0.01, 0.001, 0.0001, 0.00001],\n",
    "    'random_state': [42],\n",
    "    'batch_size': [32, 64, 128]\n",
    "}\n",
    "\n",
    "# Create an MLPClassifier\n",
    "m = MLPClassifier()\n",
    "\n",
    "# Define a scorer (use 'neg_log_loss' if you want to optimize for log loss)\n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(m, param_grid, scoring=scorer, cv=3, n_jobs=-1)\n",
    "\n",
    "# Fit the model with the training data\n",
    "grid_search.fit(XTr, YTrE)\n",
    "\n",
    "# Print the best parameters and corresponding accuracy\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Present\n",
    "In a customer faceing Document provide summery of finding and detail approach taken\n",
    "\n",
    "\n",
    "1. Upon Recieving the Data, it was immediately processed by converting all the images into jpgs, grayscaling it, locating the area in which the cursive letter was to the best of my ability, and extracting that area, and then compressing and cropping that image to a preset size, namely 500x500 in my case.  \n",
    "2. The model was then trained using a TensorFlow based neural network in orer to have a finer control over the neural network as well as increasing general speed in processing and training the model. \n",
    "3. Fine tuning would prove to be quite difficult and quite useless, as the error within the model itself isn't the model, or how it was constructed, but rather the poor quality data in addition to the lack of it. If given more time, as well as a bigger budget, a larger dataset would of definitely improved things. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Launch the Model System\n",
    "Define your production run code, This should be self susficent and require only your model pramaters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f272c6c4d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f272c6c4d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "['z']\n"
     ]
    }
   ],
   "source": [
    "def something(iP):\n",
    "    import tensorflow as tf\n",
    "    import cv2\n",
    "    import os\n",
    "    import pillow_heif\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.base import BaseEstimator, TransformerMixin\n",
    "    \n",
    "    class midCropper(BaseEstimator, TransformerMixin):\n",
    "        def fit(self, x, y=None):\n",
    "            return self\n",
    "    \n",
    "        def transform(self, x):\n",
    "            imgHolder = []\n",
    "            \n",
    "            # def compressor(n, imgHolder):\n",
    "            img = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n",
    "            contour, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            if contour:\n",
    "                x1, y1, x2, y2 = cv2.boundingRect(contour[0])\n",
    "                imgHolder.append({'yV': 1,  'xV': (np.array(Image.fromarray(img[y1:y1 + y2, x1:x2]).resize((500, 500))).flatten())})\n",
    "                #print('pog')\n",
    "            else:\n",
    "                print('WEEE WOO WEE WOO ERROR ERROR ERROR', n)\n",
    "                return None\n",
    "\n",
    "            return imgHolder\n",
    "    \n",
    "            # if not os.path.isdir(x):\n",
    "            #     imgHolder = compressor(x, imgHolder)\n",
    "            \n",
    "\n",
    "            # return imgHolder\n",
    "    \n",
    "    class preProcessor(BaseEstimator, TransformerMixin):\n",
    "        def fit(self, x, y=None):\n",
    "            return self\n",
    "    \n",
    "        def transform(self, x):\n",
    "            processed_images = []\n",
    "    \n",
    "            def r(p):\n",
    "                if (p.endswith('.HEIC') == True or p.endswith('.HEIF') == True):\n",
    "                    hF = pillow_heif.read_heif(p)\n",
    "                    i = Image.frombytes(\n",
    "                        hF.mode,\n",
    "                        hF.size,\n",
    "                        hF.data.tobytes(),\n",
    "                        \"raw\",\n",
    "                    )\n",
    "                    processed_images.append(i)\n",
    "    \n",
    "                elif (p.endswith('.PNG') == True):\n",
    "                    i = Image.open(p)\n",
    "                    i = i.convert('RGB')\n",
    "                    processed_images.append(i)\n",
    "    \n",
    "                elif (p.endswith('.JPG') == False and p.endswith('.JPEG') == False):\n",
    "                    print('Error has occurred')\n",
    "                    return None\n",
    "    \n",
    "            if os.path.isdir(x):\n",
    "                for n, fp in zip(range(x), x):\n",
    "                    r(fp)\n",
    "            else:\n",
    "                r(x)\n",
    "    \n",
    "            return processed_images\n",
    "\n",
    "    \n",
    "    def processor(input):\n",
    "        if os.path.isdir(input):\n",
    "            filePath = [os.path.join(input, fileName) for fileName in os.listdir(input)]\n",
    "        else:\n",
    "            filePath = input\n",
    "    \n",
    "        pipeline = Pipeline([\n",
    "            #('preprocess', preProcessor()),\n",
    "            ('crop', midCropper()),\n",
    "        ])\n",
    "    \n",
    "        output = pipeline.fit_transform(filePath)\n",
    "        dF = pd.DataFrame(output)\n",
    "        #print(dF)\n",
    "        xVM = np.array(list(dF['xV']))\n",
    "        xVM = xVM.reshape(xVM.shape[0], -1)\n",
    "    \n",
    "        return xVM\n",
    "    \n",
    "    def LARP(iP, mP='m2.pb'):\n",
    "        lM = tf.keras.models.load_model(mP)\n",
    "    \n",
    "        xVM = processor(iP)\n",
    "    \n",
    "        PRE = lM.predict(xVM)\n",
    "    \n",
    "        lM_dict = {l: i for l, i in enumerate('abcdefghijklmnopqrstuvwxyz')}\n",
    "        PREP = np.array([lM_dict[np.argmax(row)] for row in PRE])\n",
    "        return PREP.tolist()\n",
    "    \n",
    "    print(LARP(iP))\n",
    "\n",
    "something('temp/1/a.JPG')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
